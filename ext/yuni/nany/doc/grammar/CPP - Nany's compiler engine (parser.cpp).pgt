##TEMPLATE-NAME 'C++ - Nany2Cpp: parser.cpp (modified from Kessels C engine)'
##LANGUAGE 'C++'
##ENGINE-NAME 'Nany2Cpp engine'
##AUTHOR 'Bruno Borri'
##FILE-EXTENSION 'cpp'
##NOTES
This template creates a source file to include in the Nany compiler.
The ParseFile() function must be called to run the process.
##END-NOTES
##ID-SEPARATOR '_'
##ID-SYMBOL-PREFIX 'Symbol'
##ID-RULE-PREFIX 'Rule'
##DELIMITER ','
//
//
// Generated by Nany's parser.cpp engine (modified from Kessel's C engine)
//
//


#include <cstdio>
#include <cstdlib>
#include <cwchar>                  // wchar_t
#include <sys/stat.h>
#include <cassert>
#include <iostream>

extern "C"
{
#include "engine.h"                 // The Kessels engine.
#include "grammar.h"                // Generated by GOLD.
} // extern "C"
#include "parser.h"
#include "yuni/yuni.h"
#include "yuni/core/string.h"
#include "yuni/uuid/uuid.h"
#include "../ast/all.h"
#include "../typing/type.h"


#define TRIMREDUCTIONS 0            // 0=off, 1=on




// Forward declaration of generic routine for rule propagation
template<class NodeT = Nany::Ast::Node>
class AstParse
{
public:
	static NodeT* Child(TokenStruct* parent, unsigned int index);
};

// Forward declaration of generic routine for symbol management
static const wchar_t* GetChildSymbol(TokenStruct* token, unsigned int index);



///// Helper subroutines



// Make a readable copy of a string. All characters outside 32...127 are
// displayed as a HEX number in square brackets, for example "[0A]".
static void ReadableString(wchar_t* input, wchar_t* output, long width)
{
	// Sanity check.
	if (!output || !input || (width < 1))
		return;
	output[0] = 0;

	long i1 = 0;
	long i2 = 0;
	while ((i2 < width - 1) && (input[i1] != 0))
	{
		if ((input[i1] >= 32) && (input[i1] <= 127))
		{
			output[i2++] = input[i1];
		}
		else
		{
			if (width - i2 > 4)
			{
				char s1[BUFSIZ];
				sprintf(s1, "%02X", input[i1]);
				output[i2++] = '[';
				output[i2++] = s1[0];
				output[i2++] = s1[1];
				output[i2++] = ']';
			}
		}
		i1++;
	}
	output[i2] = 0;
}




///// Rule subroutines




##RULE-TABLE
##RULES
// %Description%
Nany::Ast::Node* %ID%(TokenStruct* token)
{
	// Not yet implemented !
	assert(false && "%ID%: Not yet implemented !");
}




##END-RULES
##END-RULE-TABLE
///// Rule jumptable




##RULE-TABLE
Nany::Ast::Node* (*RuleJumpTable[])(TokenStruct* token) =
{
##RULES
	// %Index%. %Description%
	%ID%%Delimiter%
##END-RULES
};
##END-RULE-TABLE

///// Rule subroutine template


template<class NodeT>
NodeT* AstParse<NodeT>::Child(TokenStruct* parent, unsigned int index)
{
	assert(parent && "ParseChild: invalid parent");
	// Make sure the child index is not out of bounds
	assert(index < (unsigned int) Grammar.RuleArray[parent->ReductionRule].SymbolsCount
		&& "ParseChild: index out of bounds !");

	TokenStruct* child = parent->Tokens[index];
	assert(child && "ParseChild: Invalid child");
	// Make sure the child is a rule
	assert(child->ReductionRule >= 0 && "ParseChild must be called on a rule !");

	// Call the rule's subroutine via the RuleJumpTable and return the node
	Nany::Ast::Node* genericNode = RuleJumpTable[child->ReductionRule](child);

	// Cast into the specific wanted node type
	NodeT* specificNode = dynamic_cast<NodeT*>(genericNode);
	assert(specificNode == genericNode && "ParseChild : Invalid dynamic cast !");

	return specificNode;
}


static const wchar_t* GetChildSymbol(TokenStruct* parent, unsigned int index)
{
	assert(parent && "GetChildSymbol: invalid parent");
	// Make sure the child index is not out of bounds
	assert(index < (unsigned int) Grammar.RuleArray[parent->ReductionRule].SymbolsCount
		&& "GetChildSymbol: index out of bounds !");

	TokenStruct* child = parent->Tokens[index];
	assert(child && "GetChildSymbol: invalid child");
	// Make sure the child is a symbol
	assert(child->ReductionRule < 0 && "GetChildSymbol must be called on a symbol !");

	return child->Data;
}




///// Main



// Load input file from disk into memory.
static wchar_t* LoadInputFile(const char *fileName)
{
	// Sanity check.
	if (!fileName || !*fileName)
		return nullptr;

	// Open the file.
	FILE* inFile = fopen(fileName, "rb");
	if (!inFile)
	{
		std::cerr << "Could not open input file: " << fileName << std::endl;
		return nullptr;
	}

	// Get the size of the file.
	struct stat statbuf;
	if (fstat(fileno(inFile), &statbuf) != 0)
	{
		std::cerr << "Could not stat() the input file: " << fileName << std::endl;
		fclose(inFile);
		return NULL;
	}

	// Allocate memory for the input.
	char* buf1 = new char[statbuf.st_size + 1];
	wchar_t* buf2 = new wchar_t[sizeof(wchar_t) * (statbuf.st_size + 1)];
	if (!buf1 || !buf2)
	{
		std::cerr << "Not enough memory to load the file: " << fileName << std::endl;
		fclose(inFile);
		if (buf1 != NULL)
			delete buf1;
		if (buf2 != NULL)
			delete buf2;
		return NULL;
	}

	// Load the file into memory.
	size_t bytesRead = fread(buf1, 1, statbuf.st_size, inFile);
	buf1[bytesRead] = '\0';

	// Close the file.
	fclose(inFile);

	// Exit if there was an error while reading the file.
	if (bytesRead != (size_t)statbuf.st_size)
	{
		std::cerr << "Error while reading input file: " << fileName << std::endl;
		delete buf1;
		delete buf2;
		return nullptr;
	}

	// Convert from ASCII to Unicode.
	for (unsigned long i = 0; i <= bytesRead; i++)
		buf2[i] = buf1[i];
	delete buf1;

	return buf2;
}



static void ShowErrorMessage(TokenStruct* token, int result)
{
	switch (result)
	{
		case PARSELEXICALERROR:
			std::cerr << "Lexical error";
			break;
		case PARSECOMMENTERROR:
			std::cerr << "Comment error";
			break;
		case PARSETOKENERROR:
			std::cerr << "Tokenizer error";
			break;
		case PARSESYNTAXERROR:
			std::cerr << "Syntax error";
			break;
		case PARSEMEMORYERROR:
			std::cerr << "Out of memory";
			break;
	}
	if (token)
		std::cerr << " at line " << token->Line << " column " << token->Column;
	std::cerr << "." << std::endl;

	if (result == PARSELEXICALERROR)
	{
		if (token->Data != NULL)
		{
			wchar_t s1[BUFSIZ];
			ReadableString(token->Data, s1, BUFSIZ);
			std::wcerr << L"The grammar does not specify what to do with '" << s1 << L"'." << std::endl;
		}
		else
		{
			std::cerr << "The grammar does not specify what to do." << std::endl;
		}
	}
	if (result == PARSETOKENERROR)
	{
		std::cerr << "The tokenizer returned a non-terminal." << std::endl;
	}
	if (result == PARSECOMMENTERROR)
	{
		std::cerr << "The comment has no end, it was started but not finished." << std::endl;
	}
	if (result == PARSESYNTAXERROR)
	{
		if (token->Data)
		{
			wchar_t s1[BUFSIZ];
			ReadableString(token->Data, s1, BUFSIZ);
			std::wcerr << L"Encountered '" << s1 << L"', but expected ";
		}
		else
		{
			std::cerr << "Expected ";
		}

		for (unsigned int i = 0; i < (unsigned int) Grammar.LalrArray[token->Symbol].ActionCount; ++i)
		{
			unsigned int symbol = (unsigned int) Grammar.LalrArray[token->Symbol].Actions[i].Entry;
			if (Grammar.SymbolArray[symbol].Kind == SYMBOLTERMINAL)
			{
				if (i > 0)
				{
					std::cerr << ", ";
					if (i + 2 >= (unsigned int) Grammar.LalrArray[token->Symbol].ActionCount)
						std::cerr << "or ";
				}
				std::wcerr << L'\'' << Grammar.SymbolArray[symbol].Name << L'\'';
			}
		}
		std::cerr << "." << std::endl;
	}
}



Nany::Ast::Node* parseFile(const char* filePath)
{
	Nany::Ast::Node* tree = nullptr;

	// Load the inputfile into memory.
	wchar_t* inputBuf = LoadInputFile(filePath);
	if (!inputBuf)
	{
		std::cerr << "\"" << filePath << "\" could not be opened." << std::endl;
		return tree;
	}


	// Run the Parser.
	TokenStruct* token;
	int parseResult = Parse(inputBuf, wcslen(inputBuf), TRIMREDUCTIONS, &token);

	// Interpret the results.
	if (parseResult != PARSEACCEPT)
	{
		ShowErrorMessage(token, parseResult);
	}
	else
	{
		// Start execution by calling the subroutine of the first Token on
		// the TokenStack. It's the "Start Symbol" that is defined in the
		// grammar.
		tree = RuleJumpTable[token->ReductionRule](token);
	}

	// Cleanup.
	DeleteTokens(token);
	delete inputBuf;
	return tree;
}


